"0",""
"0","############### Bigrams ###########"
"0",""
"0","# Bigram undesirable words"
"0","bigramundesirable <- tibble(word = c(NA,""ago"",""vaccine"",""covid"",""thousand"",""ten"",""twond"",""twenty"",""threerd"",""onest"",""april"",""coil"",""fourteen"",""menstrual"",""2"",""19"",""january"",""eighteen""))"
"0",""
"0","# Lemmatize text"
"0","df_clean$lemword<-lemmatize_strings(df_clean$trimother, dictionary = lexicon::hash_lemmas[!token%in%c(""shot"", ""spotting""), ])"
"0",""
"0","# Tokenizing by adjacent words, bigrams."
"0","df_bigram <- df_clean %>%"
"0","  unnest_tokens(bigram, lemword, token = ""ngrams"", n = 2) "
"0",""
"0","#Seperate bigrams "
"0","bigrams_separated <- df_bigram %>%"
"0","  separate(bigram, c(""word1"", ""word2""), sep = "" "")"
"0",""
"0","#Remove stop words"
"0","bigrams_filtered <- bigrams_separated %>%"
"0","  filter(!word1 %in% stop_words$word) %>%"
"0","  filter(!word2 %in% stop_words$word) %>%"
"0","  filter(!word1 %in% bigramundesirable$word) %>%"
"0","  filter(!word2 %in% bigramundesirable$word)"
"0",""
"0",""
"0","# Reunite bigrams"
"0","bigrams_united <- bigrams_filtered %>%"
"0","  unite(bigram, word1, word2, sep = "" "")"
"0",""
"0","# A few don't make sense when put back together, so remove them: "
"0","bigrampairremove <- tibble(word = c(""abdominal abdominal""))"
"0","bigrams_united <- bigrams_united %>%"
"0","  filter(!bigram %in% bigrampairremove$word) "
"0",""
"0","# Visualise top bigram"
"0","graph_topbiLEM<-bigrams_united %>%"
"0","  # remove stop words"
"0","  dplyr::count(bigram, sort = TRUE) %>%"
"0","  slice_max(n, n = 20) %>%"
"0","  mutate(bigram = reorder(bigram, n)) %>%"
"0","  # put `n` on the x-axis and `lemword` on the y-axis"
"0","  ggplot(aes(n, bigram)) +"
"0","  geom_col(fill=""#7AC5CD"")+"
"0","  labs(title="""",x=""Number of Times Used"", y=""Bigram"")+"
"0","    theme(axis.text=element_text(size=12))+theme_bw()"
"0",""
